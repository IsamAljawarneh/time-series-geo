{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.jars.packages': 'org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0', 'spark.jars': '/home/isam/Desktop/zeppelin-packages/magellan-1.0.5-s_2.11.jar', 'spark.jars.excludes': 'org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11', 'spark.dynamicAllocation.enabled': False}, 'kind': 'spark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%local \n",
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.jars.packages\": \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.2.0,com.esri.geometry:esri-geometry-api:1.2.1,commons-io:commons-io:2.6,org.apache.spark:spark-streaming_2.11:2.2.0,org.apache.spark:spark-sql_2.11:2.2.0\",\n",
    "        \"spark.jars\":\"/home/isam/Desktop/zeppelin-packages/magellan-1.0.5-s_2.11.jar\",\n",
    "        \"spark.jars.excludes\": \"org.scala-lang:scala-reflect,org.apache.spark:spark-tags_2.11\",\n",
    "        \"spark.dynamicAllocation.enabled\": false\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>None</td><td>spark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "/**\n",
    " * @Description: SAOS integrated with ApproxSPE \n",
    " * @author: Isam Al Jawarneh\n",
    " * @date: 24/10/2021\n",
    " */\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import util.control.Breaks._\n",
      "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
      "import org.apache.spark.sql.functions.col\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.rdd.RDD\n",
      "import org.apache.spark.SparkContext\n",
      "import org.apache.spark.SparkConf\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SQLImplicits\n",
      "import org.apache.spark.sql.functions.from_json\n",
      "import org.apache.spark.sql.functions._\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.spark.sql.Dataset\n",
      "import org.apache.spark.sql.ForeachWriter\n",
      "import magellan._\n",
      "import magellan.index.ZOrderCurve\n",
      "import magellan.{Point, Polygon}\n",
      "import org.apache.spark.sql.magellan.dsl.expressions._\n",
      "import org.apache.spark.sql.Row\n",
      "import org.apache.spark.sql.types._\n",
      "import org.apache.spark.sql.SparkSession\n",
      "import org.apache.spark.sql.streaming.OutputMode\n",
      "import org.apache.spark.sql.types.{DoubleType, StringType, StructField, StructType}\n",
      "import org.apache.spark.sql.streaming._\n",
      "import org.apache.spark.sql.streaming.Trigger\n",
      "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
      "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
      "import org.apache.spark.sql.SQLContext\n",
      "import org.apache.log4j.{Level, Logger}\n",
      "import scala.collection.mutable\n",
      "import scala.concurrent.duration.Duration\n",
      "import java.io.{BufferedWriter, FileWriter}\n",
      "import org.apache.commons.io.FileUtils\n",
      "import java.io.File\n",
      "import scala.collection.mutable.ListBuffer\n",
      "import java.time.Instant\n",
      "import org.apache.spark.sql.DataFrame\n",
      "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
      "import org.apache.kafka.common.serialization.StringDeserializer\n"
     ]
    }
   ],
   "source": [
    "import util.control.Breaks._\n",
    "import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "//import org.apache.spark.util.random.XORShiftRandom\n",
    "import org.apache.spark.sql.functions.col\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SQLImplicits\n",
    "import org.apache.spark.sql.functions.from_json\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.Dataset\n",
    "import org.apache.spark.sql.ForeachWriter\n",
    "import magellan._\n",
    "import magellan.index.ZOrderCurve\n",
    "import magellan.{Point, Polygon}\n",
    "\n",
    "import org.apache.spark.sql.magellan.dsl.expressions._\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.streaming.OutputMode\n",
    "import org.apache.spark.sql.types.{\n",
    "  DoubleType,\n",
    "  StringType,\n",
    "  StructField,\n",
    "  StructType\n",
    "}\n",
    "import org.apache.spark.sql.streaming._\n",
    "import org.apache.spark.sql.streaming.Trigger\n",
    "import org.apache.spark.sql.execution.streaming.MemoryStream\n",
    "import org.apache.spark.sql.functions.{collect_list, collect_set}\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import scala.collection.mutable\n",
    "import scala.concurrent.duration.Duration\n",
    "import java.io.{BufferedWriter, FileWriter}\n",
    "import org.apache.commons.io.FileUtils\n",
    "import java.io.File\n",
    "import scala.collection.mutable.ListBuffer\n",
    "import java.time.Instant\n",
    "//import org.apache.spark.util.CollectionAccumulator\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schemaNYCshort: org.apache.spark.sql.types.StructType = StructType(StructField(id,StringType,false), StructField(lat,DoubleType,false), StructField(lon,DoubleType,false), StructField(time,StringType,false), StructField(speed,DoubleType,false))\n"
     ]
    }
   ],
   "source": [
    "val schemaNYCshort = StructType(Array(\n",
    "    StructField(\"id\", StringType, false),\n",
    "    StructField(\"lat\", DoubleType, false),\n",
    "    StructField(\"lon\", DoubleType, false),\n",
    "    StructField(\"time\", StringType, false),\n",
    "    StructField(\"speed\", DoubleType, false)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geohashUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,ArrayType(StringType,true),Some(List(ArrayType(org.apache.spark.sql.types.ZOrderCurveUDT@e7320b8,true))))\n"
     ]
    }
   ],
   "source": [
    "val geohashUDF = udf{(curve: Seq[ZOrderCurve]) => curve.map(_.toBase32())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trips: org.apache.spark.sql.DataFrame = [id: string, lat: double ... 4 more fields]\n",
      "ridesGeohashed: org.apache.spark.sql.DataFrame = [id: string, lat: double ... 6 more fields]\n",
      "warning: there was one deprecation warning; re-run with -deprecation for details\n",
      "explodedRidesGeohashed: org.apache.spark.sql.DataFrame = [id: string, lat: double ... 7 more fields]\n"
     ]
    }
   ],
   "source": [
    "val trips =spark.read.format(\"csv\").option(\"header\", \"true\").schema(schemaNYCshort).csv(\"/home/isam/Desktop/time-series/data/china/trips/\").withColumn(\"point\", point($\"lat\",$\"lon\"))\n",
    "val ridesGeohashed = trips.withColumn(\"index\", $\"point\" index  30).withColumn(\"geohashArray1\", geohashUDF($\"index.curve\"))//.select($\"id\", $\"vendorId\", $\"point\",$\"geohashArray\",$\"Trip_distance\")\n",
    "val explodedRidesGeohashed = ridesGeohashed.explode(\"geohashArray1\", \"geohash\") { a: mutable.WrappedArray[String] => a }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+------------------+------------------------+-----+---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n",
      "|id |lat               |lon               |time                    |speed|point                                        |index                                                                                                                                                  |geohashArray1|geohash|\n",
      "+---+------------------+------------------+------------------------+-----+---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n",
      "|0  |114.03179899999999|22.524798999999998|2014-10-22T02:54:30.000Z|42.0 |Point(114.03179899999999, 22.524798999999998)|[[ZOrderCurve(114.027099609375, 22.52197265625, 114.0380859375, 22.5274658203125, 30, -1872931849332850688, 111001100000001000000010011010),Contains]] |[ws104u]     |ws104u |\n",
      "|0  |114.038696        |22.5315           |2014-10-22T02:54:37.000Z|52.0 |Point(114.038696, 22.5315)                   |[[ZOrderCurve(114.0380859375, 22.5274658203125, 114.049072265625, 22.532958984375, 30, -1872931454195859456, 111001100000001000000010110001),Contains]]|[ws105j]     |ws105j |\n",
      "+---+------------------+------------------+------------------------+-----+---------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explodedRidesGeohashed.show(2,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res7: Long = 1155653\n"
     ]
    }
   ],
   "source": [
    "explodedRidesGeohashed.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "explodedRidesGeohashed.select($\"id\",$\"lon\",$\"lat\",$\"geohash\",$\"speed\",$\"time\").coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\",true).save(\"/home/isam/Desktop/time-series/data/china/ShenzhenTripsGeo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import org.apache.spark.sql.functions._\n",
      "+------------------+\n",
      "|   skewness(speed)|\n",
      "+------------------+\n",
      "|1.3405139822371657|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "explodedRidesGeohashed.agg(skewness(explodedRidesGeohashed(\"speed\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Spark",
   "language": "",
   "name": "sparkkernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
